{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CASA0018_7_lab_solution.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10CyRk0ERz-XSHFjxJjSbeoswng29yXS6",
      "authorship_tag": "ABX9TyOq56K+LIsqi8PiL4WUPmmp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djdunc/casa0018/blob/main/Week7/CASA0018_7_lab_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uodql-ykl3YN"
      },
      "source": [
        "\n",
        "# **Workshop 7 - Time Series Forecasting with Recurrent Neural Networks**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ3n9-wAX6g0"
      },
      "source": [
        "First set up the necessary Python imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPvfePGXx-1"
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator # Generates batches for sequence data\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from google.colab import files\n",
        "import tensorflow as tf\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp4CyKwG4EG1"
      },
      "source": [
        "# Set Seeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfPoNYRT4NLe"
      },
      "source": [
        "# Set seed for reproducibility\r\n",
        "seed = 1234\r\n",
        "np.random.seed(seed)\r\n",
        "tf.random.set_seed(seed)\r\n",
        "tf.compat.v1.random.set_random_seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ose5yz7LkDg8"
      },
      "source": [
        "# Load sensor data from csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqoN1gmCwwbr"
      },
      "source": [
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuNTfa0CxIrA"
      },
      "source": [
        "# Load into a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gCqjkz9xPnw"
      },
      "source": [
        "df = pd.read_csv('light-2000.csv')\n",
        "print(len(df))\n",
        "print(df)\n",
        "df.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqxeuxOphGvJ"
      },
      "source": [
        "\n",
        "Split the data into train and test (10%) subsets.So rather than splitting the data into train and test dataset using traditional train_test_split function from sklearn, here we’ll split the dataset using simple python libraries to better understand the process going under the hood.\n",
        "First, we’ll check the length of the data frame and use 10 percent of the training data to test our model. Now if we multiply the length of the data frame with test_percent and round the value (as we are using for indexing purpose) we’ll get the index position i.e., test_index. Last, we’ll split the train and test data using the test_index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqLJEefGhFXB"
      },
      "source": [
        "print(len(df)) # 2001\n",
        "test_percent = 0.1 # 10 percent of data\n",
        "len(df)*test_percent # 200.1\n",
        "test_point = np.round(len(df)*test_percent) # 200\n",
        "test_index = int(len(df) - test_point) #1801\n",
        "train = df.iloc[:test_index]\n",
        "test = df.iloc[test_index:]\n",
        "\n",
        "print(len(train))\n",
        "print(len(test))\n",
        "\n",
        "#plt.xlim(0, 50)\n",
        "#plt.ylim(-5, 5)\n",
        "#train.plot()\n",
        "#test.plot()\n",
        "plt.plot(train)\n",
        "plt.plot(test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlMsxwcmh1CC"
      },
      "source": [
        "We need to normalise the data in the range 0-1. We use a scaler to determine the max and min of the training set and then use the scale to scale both the training a test data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szt4XQKjh3KC"
      },
      "source": [
        "scaler = MinMaxScaler(copy=True, feature_range=(0, 1))\n",
        "scaler.fit(train)\n",
        "scaled_train = scaler.transform(train)\n",
        "scaled_test = scaler.transform(test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syrrjT8ciIV6"
      },
      "source": [
        "One problem we’ll face when using time series data is, we must transform the data into sequences of samples with input data and target data before feeding it into the model. We should select the length of the sequence data (window length) in such a way so that the model has an adequate amount of input data to generalize and predict.\n",
        "\n",
        "The model takes the previous 100 data points (one cycle) as input data and uses it to predict the next point, which is then compared to the actual target value for backpropagation and gradient descent. This process is time-consuming and difficult if we perform this manually, hence we’ll make use of the Keras Timeseries Generator which transforms the data automatically and ready to train models without heavy lifting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6aa2aCPiIzh"
      },
      "source": [
        "length =  100 #sequence length - the length of the training window\n",
        "batch_size=10\n",
        "generator = TimeseriesGenerator(data=scaled_train, targets=scaled_train, length=length, batch_size=batch_size)\n",
        "validation_generator = TimeseriesGenerator(data=scaled_test, targets=scaled_test, length=length, batch_size=batch_size)\n",
        "print(len(scaled_train)) \n",
        "print(len(scaled_test))\n",
        "print(generator.length) \n",
        "print(validation_generator.length)\n",
        "print(len(generator.data)) \n",
        "print(len(validation_generator.data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-kyA78viimN"
      },
      "source": [
        "Create a model and train it. The variable (n_features) defined stands for the number of features in the training data i.e., as we are dealing with univariate data we’ll only have one feature whereas if we are using data containing multiple features then we must specify the count of features in our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk-SAH4xii8C"
      },
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "\n",
        "n_features = 1\n",
        "rnn_model = Sequential()\n",
        "output_space = length # Same as number of time steps in the training window\n",
        "\n",
        "rnn_model.add(SimpleRNN(output_space, return_sequences=False, input_shape = (length , n_features)))\n",
        "\n",
        "#rnn_model.add(SimpleRNN(output_space, return_sequences=True, input_shape = (length , n_features)))\n",
        "#rnn_model.add(SimpleRNN(output_space))\n",
        "\n",
        "rnn_model.add(Dense(1))\n",
        "\n",
        "rnn_model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001), loss='mse')\n",
        "rnn_model.fit(generator, epochs=200, validation_data=validation_generator, callbacks=[callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AErS3mjitjC"
      },
      "source": [
        "# Testing\n",
        "\n",
        "Let’s test our model using first_eval_batch. The first_eval_batch contains the last 50 points of the scaled training data and uses these to make a prediction. The results of the predicted value and the first observation in the scaled_data is outputted below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNT9Gf1yit9B"
      },
      "source": [
        "first_eval_batch = scaled_train[-length:] # Take the last 100 points and predict the new value in the scaled_test\n",
        "first_eval_batch = first_eval_batch.reshape((1, length, n_features)) # shape the data to match the input_shape of model\n",
        "print(rnn_model.predict(first_eval_batch)) # array([[0.92780817]], dtype=float32)\n",
        "print(scaled_test[0]) # array([0.94955134])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIQsgkyYl2T1"
      },
      "source": [
        "We can also look further into the future.\n",
        "\n",
        "First, we’ll define an empty list (test_predictions) so we can append the predicted values. The second step is to define first_eval_batch i.e. the first evaluation batch that needs to be sent into the model and reshape the batch so it matches the input shape of our model. Our current_batch contains the last 100 values from the training data.\n",
        "\n",
        "Finally, we’ll define a loop that continues until it reaches the end of the test data. The predicted value gets appended to the end of current_batch and the first observation in the current_batch gets removed. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q03ZYJol21K"
      },
      "source": [
        "test_predictions = []\n",
        "\n",
        "first_eval_batch = scaled_train[-length:]\n",
        "\n",
        "current_batch = first_eval_batch.reshape(1, length, n_features)\n",
        "\n",
        "\n",
        "for i in range(len(test)):\n",
        "  current_pred = rnn_model.predict(current_batch)[0]\n",
        "\n",
        "  test_predictions.append(current_pred)\n",
        "\n",
        "  current_batch = np.append(current_batch[:, 1:, :],[[current_pred]],axis = 1)\n",
        "\n",
        "true_predictions = scaler.inverse_transform(test_predictions)\n",
        "test['RNN Predictions'] = true_predictions\n",
        "test.plot(figsize=(12,8))\n",
        "#test.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7_kpksr5Vm1"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#**Long Short-Term Memory (LSTM) Neural Networks**\n",
        "\n",
        "Now lets try a LSTM model. The code is very similar to that for a vanilla RNN except we use an LSTM layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkxRhc5J5Zpj"
      },
      "source": [
        "length = 100\n",
        "batch_size = 10\n",
        "n_features = 1\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "\n",
        "generator = TimeseriesGenerator(scaled_train, scaled_train, length=length, batch_size=batch_size)\n",
        "validation_generator = TimeseriesGenerator(scaled_test, scaled_test, length=length, batch_size=batch_size)\n",
        "\n",
        "lstm_model = Sequential()\n",
        "\n",
        "output_space = length # Same as number of time steps in the training window\n",
        "\n",
        "lstm_model.add(LSTM(output_space, input_shape=(length, n_features)))\n",
        "\n",
        "#lstm_model.add(LSTM(output_space, return_sequences=True, input_shape=(length,n_features)))\n",
        "#lstm_model.add(LSTM(output_space))\n",
        "\n",
        "lstm_model.add(Dense(1))\n",
        "\n",
        "lstm_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse')\n",
        "lstm_model.fit(generator, epochs=200, validation_data=validation_generator, callbacks=[callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1dhJLoF5jBY"
      },
      "source": [
        "**Predict and Visualise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap60LCkd5jol"
      },
      "source": [
        "test_predictions = []\n",
        "\n",
        "first_eval_batch = scaled_train[-length:]\n",
        "\n",
        "current_batch = first_eval_batch.reshape(1, length,n_features)\n",
        "\n",
        "\n",
        "for i in range(len(test)):\n",
        "  current_pred = lstm_model.predict(current_batch)[0]\n",
        "\n",
        "  test_predictions.append(current_pred)\n",
        "\n",
        "  current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis = 1)\n",
        "\n",
        "true_predictions = scaler.inverse_transform(test_predictions)\n",
        "test['LSTM Predictions'] = true_predictions\n",
        "test.plot(figsize=(12,8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lHozPKAGRpW"
      },
      "source": [
        "#**Forecasting**\r\n",
        "Let's do some  real forecasting, beyond the end of all our data (training+validation)\r\n",
        "\r\n",
        "\r\n",
        " \r\n",
        "**RNN Forecasting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtJ_5TAxE1Fr"
      },
      "source": [
        "full_scaler = MinMaxScaler(copy=True, feature_range=(0, 1))\n",
        "scaled_full_data = full_scaler.fit_transform(df)\n",
        "\n",
        "forecast = []\n",
        "\n",
        "first_eval_batch = scaled_full_data[-length:]\n",
        "\n",
        "current_batch = first_eval_batch.reshape(1, length, n_features)\n",
        "\n",
        "\n",
        "for i in range(100):\n",
        "  current_pred = rnn_model.predict(current_batch)[0]\n",
        "\n",
        "  forecast.append(current_pred)\n",
        "\n",
        "  current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbvK4esPEzcz"
      },
      "source": [
        "**Plot the RNN Forecasted values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOavrmhfFSgc"
      },
      "source": [
        "forecast = full_scaler.inverse_transform(forecast)\n",
        "\n",
        "forecast_index = np.arange(2001, 2101, step=1)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(df.index, df['light'])\n",
        "plt.plot(forecast_index, forecast)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKJ88bB1FPvz"
      },
      "source": [
        "**LSTM Forecasting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1rOa97hFT4H"
      },
      "source": [
        "forecast = []\n",
        "\n",
        "first_eval_batch = scaled_full_data[-length:]\n",
        "\n",
        "current_batch = first_eval_batch.reshape(1, length, n_features)\n",
        "\n",
        "\n",
        "for i in range(100):\n",
        "  current_pred = lstm_model.predict(current_batch)[0]\n",
        "\n",
        "  forecast.append(current_pred)\n",
        "\n",
        "  current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2kfPKizGPj0"
      },
      "source": [
        "**Plot the LSTM Forecasted Values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK2S7q91FT3a"
      },
      "source": [
        "forecast = full_scaler.inverse_transform(forecast)\n",
        "\n",
        "forecast_index = np.arange(2001, 2101, step=1)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(df.index, df['light'])\n",
        "plt.plot(forecast_index, forecast)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}