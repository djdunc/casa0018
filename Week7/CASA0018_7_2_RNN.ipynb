{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CASA0018_7_2_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Uodql-ykl3YN"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNafcmXpaANe101gPmTVpDg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djdunc/casa0018/blob/main/Week7/CASA0018_7_2_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uodql-ykl3YN"
      },
      "source": [
        "\n",
        "# **Time Series Forecasting with Recurrent Neural Networks**\n",
        "\n",
        "\n",
        "This tutorial explores how to code simple Recurrent Neural Networks, both vanilla and LSTM, using the tensorflow libraries and test them against a trivial data set.\n",
        "\n",
        "The tutorial leans heavily on the following source:\n",
        "\n",
        "https://medium.com/towards-artificial-intelligence/beginners-guide-to-timeseries-forecasting-with-lstms-using-tensorflow-and-keras-364ea291909b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du_QkgMAIgAN"
      },
      "source": [
        "# Set Up\n",
        "\n",
        "First set up the necessary Python imports and set up Tensor Board which provides a visual output of the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPvfePGXx-1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime, os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator # Generates batches for sequence data\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,SimpleRNN,LSTM\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "#%load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "\n",
        "\n",
        "# Clear any logs from previous runs\n",
        "%rm -rf ./logs/\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IXTNVEil1G1"
      },
      "source": [
        "# Set Seeds\r\n",
        "\r\n",
        "Attempt to make the data reproduceable from run to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLoSs4nfl7Az"
      },
      "source": [
        "# Set seed for reproducibility\n",
        "seed = 1\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "#tf.compat.v1.random.set_random_seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWHD8AH-75z7"
      },
      "source": [
        "# Create Data\n",
        "\n",
        "Create sine function data. We’ll use the NumPy linspace to generate x values ranging between 0 and 60 and NumPy sine function to generate sine values to the corresponding x. We also add a linear componenent to the sine data to generate a trend. \n",
        "\n",
        "We then add some noise to the data. Finally, we visualize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_X9XnsMg7o6"
      },
      "source": [
        "x = np.linspace(0, 60, 601)  # in radians\n",
        "y = np.sin(x) + 0.02*x\n",
        "\n",
        "# Plot our data\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(x,y)\n",
        "plt.show()\n",
        "\n",
        "y += 0.2*np.random.randn(*y.shape) #Add some noise to the data to make it more realistic\n",
        "\n",
        "# Plot our data\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(x, y, 'b.')\n",
        "plt.show()\n",
        "\n",
        "# Define a dataframe using x and y values.\n",
        "df = pd.DataFrame(data=y,index=x,columns=['Sine'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqxeuxOphGvJ"
      },
      "source": [
        "# Split Data\n",
        "Split the data into train and validate subsets. The train dataset is used for training the model and the validate data set is used to validate the model against unseen data as it undergoes training. Normally there is a third unseen data set used for testing the trained model. However, the time series used here is sufficiently predictable to test 'by eye'.\n",
        "\n",
        "First, we’ll check the length of the data frame and use a fraction of the data to validate our model. Now if we multiply the length of the data frame with test_percent and round the value (as we are using for indexing purpose) we’ll get the index position i.e., test_index. Last, we’ll split the train and test data using the test_index.\n",
        "\n",
        "For simplicity the validation set is referred to as 'test', but remember it is actually used for validation of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqLJEefGhFXB"
      },
      "source": [
        "test_pecent = 0.16667 \n",
        "len(df)*test_pecent\n",
        "test_point = np.round(len(df)*test_pecent) \n",
        "test_index = int(len(df) - test_point) \n",
        "train = df.iloc[:test_index]\n",
        "test = df.iloc[test_index:]\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.xlim(0, 60)\n",
        "\n",
        "print(len(df)) # 601\n",
        "print(len(train))\n",
        "print(len(test))\n",
        "#train.plot()\n",
        "#test.plot()\n",
        "plt.plot(train)\n",
        "plt.plot(test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlMsxwcmh1CC"
      },
      "source": [
        "# Normalise\n",
        "We need to normalise the data in the range 0-1. We use a scaler to determine the max and min of the training set and then use the scale to scale both the training a validation data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szt4XQKjh3KC"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train)\n",
        "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
        "scaled_train = scaler.transform(train)\n",
        "scaled_test = scaler.transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syrrjT8ciIV6"
      },
      "source": [
        "# Time Series Generator\n",
        "One problem we’ll face when using time series data is, we must transform the data into sequences of samples with input data and target data before feeding it into the model. We should select the length of the data sequence (window length) in such a way so that the model has an adequate amount of input data to generalize and predict i.e. in this situation we must feed the model at least with one cycle of sine wave values.\n",
        "\n",
        "The model takes the previous 50 data points (one window) as input data and uses it to predict the next point, which is then compared to the actual target value for backpropagation and gradient descent. \n",
        "\n",
        "This process is time-consuming and difficult if we perform this manually, hence we’ll make use of the Keras Timeseries Generator which transforms the data automatically and ready to train models without heavy lifting.\n",
        "\n",
        "We can see that the length of the scaled_train is 501 and the length of the generator is 451(501–50) i.e. if we perform the tuple unpacking of the generator function using X,y as variables, X comprises the 50 data points (training window) and y contains the 51st data point which the model uses for the prediction target. i.e. X0 = values at timesteps 0-49 and y0 = value at timestep 50; X1 = values at timesteps 1-50 and y1 = value at timestep 51; ... X451 = values at timesteps 451-500 and y451 = value at timestep 501.\n",
        "\n",
        "We also create a validation_generator that operates on the validation data set (scaled_test in the code). The validator_generator is not used to train the model. Instead, it is used after each epoch to validate how well the current model fits an **unseen** data set.\n",
        "\n",
        "The batch size indicates how many sequences (windows) are seen by the network before the network weights are updated. A batch size of 1 means the weights are updated after every sequence (window) is input to the network. In this case a batch size of 10 is used, which speeds up training.\n",
        "\n",
        "An epoch is one complete pass through the training data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6aa2aCPiIzh"
      },
      "source": [
        "length = 50 # sequence length - the length of the training window\n",
        "batch_size = 10\n",
        "\n",
        "generator = TimeseriesGenerator(data=scaled_train, targets=scaled_train, length=length, batch_size=batch_size)\n",
        "\n",
        "validation_generator = TimeseriesGenerator(scaled_test, scaled_test, length=length, batch_size=batch_size)\n",
        "\n",
        "print(len(scaled_train)) # 501\n",
        "print(len(generator)) # 451"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-kyA78viimN"
      },
      "source": [
        "# Create a model and train it\n",
        "The code to create a RNN is similar to that for earlier NNs you have already seen, although here we use a SimpleRNN layer.\n",
        "\n",
        "The variable (n_features) defined stands for the number of features in the training data i.e., as we are dealing with univariate data we’ll only have one feature whereas if we are using multivariate data containing multiple features then we must specify the count of features in our data.\n",
        "\n",
        "early_stop is a callback that stops the training if the validation loss fails to decrease over a number of epochs specified by the patience value (15)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk-SAH4xii8C"
      },
      "source": [
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\"rnn\")\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "\n",
        "n_features = 1\n",
        "rnn_model = Sequential()\n",
        "output_space = length # Same as number of time steps in the training window\n",
        "rnn_model.add(SimpleRNN(output_space, return_sequences=False, input_shape = (length, n_features)))\n",
        "#rnn_model.add(SimpleRNN(output_space))\n",
        "rnn_model.add(Dense(1))\n",
        "rnn_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001), loss='mse')\n",
        "rnn_model.summary()\n",
        "rnn_model.fit(generator, epochs=200, validation_data=validation_generator, callbacks=[early_stop, tensorboard])\n",
        "%tensorboard --logdir logs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AErS3mjitjC"
      },
      "source": [
        "# Predict Next Value\n",
        "Let’s test our model using first_eval_batch. The first_eval_batch contains the last 50 points of the scaled training data and uses these to make a prediction. The results of the predicted value and the first observation in the scaled_data is commented below for understanding. Our model predicts the next point as 0.567 whereas the original value is 0.599. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNT9Gf1yit9B"
      },
      "source": [
        "first_eval_batch = scaled_train[-length:] # Take the last 50 points and predict the new value in the scaled_test\n",
        "first_eval_batch = first_eval_batch.reshape((1, length, n_features)) # shape the data to match the input_shape of model\n",
        "print(rnn_model.predict(first_eval_batch))\n",
        "print(scaled_test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIQsgkyYl2T1"
      },
      "source": [
        "# Forecasting Using The Validation Data\n",
        "We can automate this processing of generating batches of data for evaluation from validation data. We’ll define a function that does the heavy lifting for us.\n",
        "\n",
        "First, we’ll define an empty list (test_predictions) so we can append the predicted values. The second step is to define (first_eval_batch) i.e., the first evaluation batch that needs to be sent into the model and reshape the batch so it matches the input shape of our model. Our current_batch contains all the last 50 values from the training data.\n",
        "\n",
        "Finally, we’ll define a loop that continues until it reaches the end of the test data. The predicted value gets appended to the (current_batch) and the first observation in the current_batch gets removed. i.e., our current_batch contains 50 values of which 49 are from the training data and 50th value is the model predicted value which gets appended.\n",
        "\n",
        "You should be aware that this is not a true test as the validation data set was used to determine early stopping in other words the model was trained to minimise loss against the validation data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q03ZYJol21K"
      },
      "source": [
        "test_predictions = []\n",
        "\n",
        "first_eval_batch = scaled_train[-length:]\n",
        "\n",
        "current_batch = first_eval_batch.reshape(1, length, n_features)\n",
        "\n",
        "\n",
        "for i in range(len(test)):\n",
        "  current_pred = rnn_model.predict(current_batch)[0]\n",
        "\n",
        "  test_predictions.append(current_pred)\n",
        "\n",
        "  current_batch = np.append(current_batch[:,1:,:], [[current_pred]], axis = 1)\n",
        "\n",
        "true_predictions = scaler.inverse_transform(test_predictions)\n",
        "test['RNN Predictions'] = true_predictions\n",
        "test.plot(figsize=(12,8))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7_kpksr5Vm1"
      },
      "source": [
        "\n",
        "There is little difference between the predicted value and the original sine wave value at the beginning as the first sequence we sent for our model evaluation comprises the last 50 values from training data however as the loop continues the predicted values get appended to the sequence that is fed into the model which causes a progressive deviation of the predicted curve from the true values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to3VwwYtVNke"
      },
      "source": [
        "\r\n",
        "----------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsgjxKlPTiXJ"
      },
      "source": [
        "\n",
        "# Long Short-Term Memory (LSTM) Neural Networks\n",
        "\n",
        "Consider speech recognition. In speech recognition context is important, which requires the network to have some memory. For example consider the phrase “The sky is blue”. The context (“sky”) helps recognise the word “blue”.\n",
        "\n",
        "Now consider another example: “The field was filled with cows who seemed very happy as they ate the grass”. We are trying to identify the word \"grass\", but the clues (\"field\", \"cows\") are earlier in the phrase. The further back in time the context information is positioned, the more challenging the problem is and the longer term memory is required.\n",
        "\n",
        "LSTM have longer memory compared to vanilla RNNs. Thus LSTMs are perfect for speech recognition tasks or tasks where we have to deal with time-series data and longer term time dependencies are important.\n",
        "\n",
        "Let's apply a LSTM to our sine data. The code for the network is similar to the vanilla RNN, except we use a LSTM layer instead of SimpleRNN layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkxRhc5J5Zpj"
      },
      "source": [
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\"lstm\")\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "\n",
        "length = 50\n",
        "batch_size = 10\n",
        "\n",
        "generator = TimeseriesGenerator(scaled_train, scaled_train, length=length, batch_size=batch_size)\n",
        "validation_generator = TimeseriesGenerator(scaled_test, scaled_test, length=length, batch_size=batch_size)\n",
        "\n",
        "lstm_model = Sequential()\n",
        "\n",
        "output_space = length # Same as number of time steps in the training window\n",
        "\n",
        "lstm_model.add(LSTM(output_space, return_sequences=False, input_shape=(length, n_features)))\n",
        "#lstm_model.add(LSTM(output_space))\n",
        "lstm_model.add(Dense(1))\n",
        "\n",
        "lstm_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse')\n",
        "lstm_model.summary()\n",
        "lstm_model.fit(generator, epochs=200, validation_data=validation_generator, callbacks=[early_stop, tensorboard])\n",
        "%tensorboard --logdir logs\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1dhJLoF5jBY"
      },
      "source": [
        "# Comparing Forecasts Using The Validation Data\n",
        "\n",
        "Now let's compare the forecasts of the RNN and LSTM models using our validation data set.\n",
        "\n",
        "Again remembering this is not a true test as the validation set was used to validate our models.\n",
        "\n",
        "Before we plot the results we reverse the scaling transformations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap60LCkd5jol"
      },
      "source": [
        "test_predictions = []\n",
        "\n",
        "first_eval_batch = scaled_train[-length:]\n",
        "\n",
        "current_batch = first_eval_batch.reshape(1,length,n_features)\n",
        "\n",
        "\n",
        "for i in range(len(test)):\n",
        "  current_pred = lstm_model.predict(current_batch)[0]\n",
        "\n",
        "  test_predictions.append(current_pred)\n",
        "\n",
        "  current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis = 1)\n",
        "\n",
        "true_predictions = scaler.inverse_transform(test_predictions)\n",
        "test['LSTM Predictions'] = true_predictions\n",
        "test.plot(figsize=(12,8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lHozPKAGRpW"
      },
      "source": [
        "# Forecasting into the future\n",
        "\n",
        "Now let's look into the future, beyond our current data set. Normally, we would compare the results against our test data set that has never been seen by the network. However, as the current series is easily predictable we will rely on visual inspection. This is the first \"true\" test.\n",
        "\n",
        "Strictly speaking we should retrain both our models using the full data set (training + validation data). But to save time we use our current models.\n",
        "\n",
        "We forecast values into the future for time running from 60.1 to 65.1 for time steps of 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtJ_5TAxE1Fr"
      },
      "source": [
        "full_scaler = MinMaxScaler()\n",
        "\n",
        "scaled_full_data = full_scaler.fit_transform(df)\n",
        "\n",
        "forecast_rnn = []\n",
        "forecast_lstm = []\n",
        "\n",
        "\n",
        "first_eval_batch = scaled_full_data[-length:]\n",
        "\n",
        "current_batch_rnn = first_eval_batch.reshape(1, length, n_features)\n",
        "current_batch_lstm = first_eval_batch.reshape(1, length, n_features)\n",
        "\n",
        "\n",
        "for i in range(50):\n",
        "  current_pred_rnn = rnn_model.predict(current_batch_rnn)[0]\n",
        "  current_pred_lstm = lstm_model.predict(current_batch_lstm)[0]\n",
        "\n",
        "  forecast_rnn.append(current_pred_rnn)\n",
        "  forecast_lstm.append(current_pred_lstm)\n",
        "\n",
        "  current_batch_rnn = np.append(current_batch_rnn[:,1:,:], [[current_pred_rnn]], axis = 1)\n",
        "  current_batch_lstm = np.append(current_batch_lstm[:,1:,:], [[current_pred_lstm]], axis = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbvK4esPEzcz"
      },
      "source": [
        "# Plot the Forecasted values\r\n",
        "\r\n",
        "To plot the result we first need to reverse the scaling transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOavrmhfFSgc"
      },
      "source": [
        "forecast_rnn = full_scaler.inverse_transform(forecast_rnn)\n",
        "forecast_lstm = full_scaler.inverse_transform(forecast_lstm)\n",
        "\n",
        "forecast_index = np.arange(60.1, 65.1, step=0.1)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(df.index,df['Sine'])\n",
        "plt.plot(forecast_index, forecast_rnn)\n",
        "plt.plot(forecast_index, forecast_lstm)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}